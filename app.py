import streamlit as st
import atexit

from utils.github_client import GitHubClient
from utils.chunking import Chunker
from utils.embeddings import Embedder
from utils.embeddings import PineconeVectorStore

from agents.rag import RAGAgent
from langchain_community.chat_models import ChatOllama
from langchain_community.llms import Ollama

from agents.orchestrator import Orchestrator
from agents.github_agent import GitHubMCPAgent
from agents.github_exec_tool import GitHubExecTool

from utils.query_analysis import QueryAnalyzer
import asyncio
from utils.runner_async import AsyncRunner

def streamlit_logger(msg):
    st.info(msg)

def get_chat_llm():
    if "chat_llm" not in st.session_state:
        st.session_state.chat_llm = ChatOllama(
            model="qwen2.5:7b-instruct-q4_0",
            temperature=0.0,
            # manda opciones a Ollama para reducir huella
            model_kwargs={
                "num_ctx": 2048,      # menos memoria
                "num_thread": 4,      # CPU estable
                "keep_alive": "30s"   # descarga del VRAM/RAM si estÃ¡ inactivo
            }
        )
    return st.session_state.chat_llm

def init_query_analyzer():
    """Inicializa el analizador de consultas"""
    if 'query_analyzer' not in st.session_state:
        llm = get_chat_llm()
        st.session_state.query_analyzer = QueryAnalyzer(llm=llm, logger=streamlit_logger)
    return st.session_state.query_analyzer


def show_query_suggestions():
    """Muestra sugerencias de consultas vÃ¡lidas"""
    st.info("ğŸ’¡ **Prueba con estas consultas de ejemplo:**")
    
    suggestions = [
        "Â¿CÃ³mo instalar la librerÃ­a numpy?",
        "Â¿CuÃ¡les son las principales funcionalidades de React?",
        "Â¿QuÃ© dependencias necesita el proyecto tensorflow?",
        "Â¿CÃ³mo contribuir al repositorio de Django?",
        "Â¿CuÃ¡l es la licencia del proyecto pandas?",
        "Â¿Hay ejemplos de uso en el repositorio de scikit-learn?"
    ]
    
    cols = st.columns(2)
    for i, suggestion in enumerate(suggestions):
        col = cols[i % 2]
        with col:
            if st.button(suggestion, key=f"suggestion_{i}", use_container_width=True):
                st.session_state.user_query = suggestion
                st.rerun()

def handle_irrelevant_query(analysis):
    """Maneja consultas que no parecen relevantes"""
    st.write(analysis["razonamiento"])
    
    return False

st.set_page_config(page_title="GitHub README Processor", page_icon="ğŸ™", layout="wide")

def streamlit_logger(msg: str):
    st.info(msg)



# 1) Un Ãºnico runner vivo
if "runner" not in st.session_state:
    st.session_state.runner = AsyncRunner()

# 2) GitHub MCP -> conectar + build executor (dentro del runner)
if "github_tool" not in st.session_state:
    gh = GitHubMCPAgent()
    try:
        # ConexiÃ³n MCP (stdio) en el loop del runner
        st.session_state.runner.run(gh.connect())
    except Exception as e:
        st.warning(f"No se pudo conectar al servidor MCP todavÃ­a: {e}")
    # Whitelist compacta de tools MCP
    allowed_tools = {
        "list_pull_requests",
    }
    executor = st.session_state.runner.run(
        gh.build_executor(allowed_tools=allowed_tools, model="qwen2.5:7b-instruct", temperature=0.0, max_iterations=3)
    )
    st.session_state.gh_client = gh
    st.session_state.github_tool = GitHubExecTool(executor=executor)

# 3) RAG (BaseTool sÃ­ncrono)
"""if "rag_tool" not in st.session_state:
    rag_tool = RAGAgent()
    rag_tool.init_agent()  
    st.session_state.rag_tool = rag_tool"""

# 4) Orquestador (usa judge_llm chat; no llames .run en Streamlit)
if "orchestrator" not in st.session_state:
    judge_llm = get_chat_llm()
    orchestrator = Orchestrator(
        tools=[st.session_state.github_tool],
        llm=judge_llm,
        logger=streamlit_logger,
        timeout_s=300.0
    )
    st.session_state.orchestrator = orchestrator

# Cierre limpio al salir
def _cleanup():
    try:
        if "gh_client" in st.session_state:
            st.session_state.runner.run(st.session_state.gh_client.close())
    finally:
        if "runner" in st.session_state:
            st.session_state.runner.stop()

atexit.register(_cleanup)
st.title("ğŸ™ Procesador de README de GitHub")

# Sidebar para configuraciÃ³n
with st.sidebar:
    st.header("âš™ï¸ ConfiguraciÃ³n")
    
    # Control de confianza mÃ­nima
    min_confidence = st.slider(
        "Confianza mÃ­nima para consultas",
        min_value=0.1,
        max_value=1.0,
        value=0.2,  # MÃ¡s permisivo para README especÃ­ficos
        step=0.1,
        help="Nivel mÃ­nimo de confianza para procesar automÃ¡ticamente la consulta"
    )
    
    st.markdown("---")

# === SECCIÃ“N 1: PROCESAMIENTO DE README ===
st.header("ğŸ“‹ Procesar README")

col1, col2 = st.columns(2)
with col1:
    owner = st.text_input("Creador del repositorio (owner)", "", key="owner_input")
with col2:
    repo = st.text_input("Nombre del repositorio", "", key="repo_input")

if st.button("ğŸš€ Procesar README", type="primary"):
    if not owner or not repo:
        st.warning("Por favor, ingresa el creador y el nombre del repositorio.")
    else:
        # Descargar README
        gh_client = GitHubClient()
        readme = gh_client.fetch_readme(owner, repo)
        if not readme:
            st.error("No se pudo descargar el README.")
        else:
            st.success("âœ… README descargado correctamente.")
            
            # Guardar info del repo en session state
            st.session_state.current_repo = f"{owner}/{repo}"

            # Chunking
            with st.spinner("ğŸ“ Dividiendo README en chunks..."):
                chunker = Chunker(max_tokens=800)
                chunks = chunker.chunk(readme, overlap=100)
            st.success(f"ğŸ“„ README dividido en {len(chunks)} chunks.")

            # Embeddings
            with st.spinner("ğŸ§  Calculando embeddings..."):
                embedder = Embedder()
                embeddings = embedder.embed_chunks(chunks, normalize=True, return_with_text=True)
            st.success(f"âœ¨ Se calcularon {len(embeddings)} embeddings.")

            try:
                document = "README"
                with st.spinner("ğŸ’¾ Guardando embeddings en Pinecone..."):
                    vector_store = PineconeVectorStore(index_name="repo-text-embed-index")
                    vector_store.upsert_embeddings(embeddings, document, repo)
                st.success("ğŸ‰ Embeddings guardados en Pinecone correctamente.")
                
                # Marcar como procesado
                st.session_state.readme_processed = True
                
            except Exception as e:
                st.error(f"âŒ Error al guardar los embeddings en Pinecone: {e}")

# === SECCIÃ“N 2: CONSULTAS ===
st.header("ğŸ’¬ Consultar Repositorios")

# Info sobre repositorios disponibles
if st.session_state.get('readme_processed', False):
    st.info(f"ğŸ“ Ãšltimo README procesado: `{st.session_state.get('current_repo', 'Repositorio')}`")

st.markdown("ğŸ’¡ Puedes consultar cualquier repositorio que haya sido procesado previamente en la base de vectores.")

# Input de consulta
user_query = st.text_area(
    "âœï¸ **Escribe tu pregunta sobre cualquier repositorio:**",
    value=st.session_state.get('user_query', ''),
    height=100,
    placeholder="Ej: Â¿CÃ³mo instalar numpy? Â¿CuÃ¡les son las funcionalidades de tensorflow/tensorflow?",
    key="query_input"
)

# Boton de acciÃ³n
send_query_button = st.button("ğŸ” Enviar Consulta", type="primary")

# Procesar consulta
if send_query_button:
    if not user_query.strip():
        st.warning("âš ï¸ Por favor, escribe una pregunta.")
    else:
        # Inicializar analizador
        with st.spinner("ğŸ” Analizando consulta..."):
            analyzer = init_query_analyzer()
            
            analysis = analyzer.analyze_query(user_query)
            
            is_relevant = analyzer.is_relevant_query(analysis, min_confidence)
        
        if is_relevant:
            # 4. Procesar consulta relevante
            st.success("âœ… Consulta vÃ¡lida. Procesando...")
            
            with st.spinner("ğŸ¤– Buscando respuesta con el agente Orchestrator..."):
                try:
                    # Ejecutar consulta
                    respuesta = st.session_state.runner.run(st.session_state.github_tool._arun(user_query))
                    
                    # Mostrar respuesta
                    st.markdown("### ğŸ¯ Respuesta:")
                    st.write(respuesta)
                    
                    
                except Exception as e:
                    st.error(f"âŒ Error al procesar la consulta: {str(e)}")
        
        else:
            # 5. Manejar consulta irrelevante
            st.write(analysis["razonamiento"])

# Mostrar sugerencias si no hay consulta
if not user_query.strip():
    st.markdown("---")
    show_query_suggestions()

# Footer con informaciÃ³n
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: #666;'>
        ğŸ™ GitHub README Processor | Consulta repositorios procesados previamente | Powered by RAG + LLM
    </div>
    """, 
    unsafe_allow_html=True
)