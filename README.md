# ğŸ™ GitHub AI Assistant

Un asistente de IA basado en **Streamlit** que te permite procesar **README** de repositorios de GitHub y consultar repositorios pÃºblicos utilizando **RAG** (Retrieval-Augmented Generation) y un agente de IA.

El sistema se integra con los siguientes frameworks:

| Framework | PropÃ³sito |
|-----------|-----------|
| **Ollama** | Inferencia LLM |
| **Langchain** | Agentes de IA |
| **GitHub MCP Server** | Datos de repositorio |
| **Pinecone** | Almacenamiento vectorial |

## ğŸ“‹ Requisitos e InstalaciÃ³n

Antes de ejecutar la aplicaciÃ³n, asegÃºrate de tener lo siguiente configurado:

### ğŸ”§ Ollama y Modelo LLM

Instala y descarga un modelo LLM localmente:

- **Instalar Ollama**: https://ollama.com/docs/installation
- **Descargar el modelo** usado por la app (ejemplo):
  
  ```bash
  # Bash/WSL/macOS:
  ollama pull qwen2.5:7b-instruct-q4_0
  ```
  
  ```powershell
  # Windows PowerShell:
  ollama pull qwen2.5:7b-instruct-q4_0
  ```

> âš ï¸ **Nota**: AsegÃºrate de que el nombre del modelo coincida con el valor en `app.py` (ej: `qwen2.5:7b-instruct-q4_0`). El modelo puede ser grande y tardar en descargar.

### ğŸ³ Docker Desktop

Requerido para ejecutar el servidor GitHub MCP:

- **Descargar**: https://www.docker.com/products/docker-desktop
- **Ejecutar el servidor GitHub MCP** (requiere un GitHub PAT):
  
  ```bash
  # Bash/WSL/Linux/macOS:
  docker run --rm -i -e GITHUB_PERSONAL_ACCESS_TOKEN=$GITHUB_TOKEN ghcr.io/github/github-mcp-server --enable-command-logging --log-file /tmp/mcp.log stdio
  ```
  
  ```powershell
  # Windows PowerShell:
  docker run --rm -i -e GITHUB_PERSONAL_ACCESS_TOKEN=$env:GITHUB_TOKEN ghcr.io/github/github-mcp-server --enable-command-logging --log-file /tmp/mcp.log stdio
  ```

> ğŸ’¡ Si usas `podman` u otro runtime, adapta el comando segÃºn sea necesario.

### ğŸ”‘ GitHub Personal Access Token (PAT)

Requerido para que el servidor MCP acceda a los datos del repositorio:

- **Crear PAT**: https://github.com/settings/tokens
- **Scopes recomendados**: `repo` (y otros segÃºn sea necesario)
- **Configurar como variable de entorno**:
  
  ```bash
  # Bash/WSL:
  export GITHUB_TOKEN="ghp_XXXXXXXXXXXXXXXXXXXX"
  ```
  
  ```powershell
  # PowerShell:
  $env:GITHUB_TOKEN = "ghp_XXXXXXXXXXXXXXXXXXXX"
  ```

### ğŸ“ Pinecone API Key e Ãndice

Configura tu Ã­ndice vectorial en Pinecone:

- **Crear cuenta**: https://www.pinecone.io/
- **Crear un Ã­ndice** con dimensiÃ³n `384`
- **Configurar variable de entorno**:
  
  ```bash
  # Bash/WSL:
  export PINECONE_API_KEY="YOUR_PINECONE_API_KEY"
  ```
  
  ```powershell
  # PowerShell:
  $env:PINECONE_API_KEY = "YOUR_PINECONE_API_KEY"
  ```

> ğŸ“Œ Nombre de Ã­ndice por defecto: `rag-index` | DimensiÃ³n: `384`

### ğŸ Python y Dependencias

Instala las dependencias del proyecto:

```bash
python -m venv .venv
.\.venv\Scripts\Activate      # Windows PowerShell
pip install -r requirements.txt
```

Ejecuta la app:

```bash
streamlit run app.py
```

## ğŸ¨ Interfaz Streamlit

### ğŸ“Š DescripciÃ³n General

La pÃ¡gina principal de la app es un **panel de control de Streamlit** dividido en dos secciones principales:

| SecciÃ³n | DescripciÃ³n |
|---------|-------------|
| **Process README** | Obtiene e indexa un README de repositorio en el almacÃ©n vectorial (Pinecone) |
| **Query Repositories** | Escribe una consulta sobre un repositorio y obtÃ©n respuestas vÃ­a Orchestrator (RAG + Agente GitHub) |

---

### ğŸ“ Process README

**Entradas:**
- Propietario del repositorio (campo de texto)
- Nombre del repositorio (campo de texto)

**Acciones:**
- Haz clic en *"Process README"* para obtener el README de GitHub
- La app dividirÃ¡ el README en fragmentos, calcularÃ¡ embeddings y los cargarÃ¡ en Pinecone

**Feedback en UI:**
- Mensajes de Ã©xito/advertencia/error mediante Streamlit
- Spinners que muestran progreso en fragmentaciÃ³n, embedding y carga a Pinecone

### ğŸ” Query Repositories

**Entradas:**
- Ãrea de texto libre para escribir una pregunta sobre un repositorio

**Acciones:**
- Haz clic en *"Send query"* para ejecutar tu pregunta a travÃ©s del agente Orchestrator
- El Orchestrator puede ejecutar la herramienta RAG (bÃºsqueda vectorial) y el agente GitHub (MCP) para obtener y combinar respuestas

**Salida:**
- La respuesta final (y cualquier mensaje de error) se muestra bajo *"Answer"*

### âš™ï¸ Comportamiento Adicional

- La app intenta conectarse al servidor GitHub MCP al iniciar; los errores de conexiÃ³n aparecen como advertencias en la UI
- Utiliza un `AsyncRunner` para ejecutar tareas asincrÃ³nicas (conexiÃ³n MCP, construcciÃ³n de agentes, embeddings) sin bloquear la interfaz de Streamlit
- Para depuraciÃ³n, la app registra mensajes mediante el logger de Streamlit (aparecen como mensajes en la UI)
- El modelo LLM y herramientas de agente permitidas se configuran en `app.py` â€” cÃ¡mbilos si es necesario para experimentos

## ğŸ¤– Agentes y Herramientas

Esta secciÃ³n describe los tres componentes principales del agente en el sistema (**RAGAgent**, **GitHubAgent** y **Orchestrator**), cÃ³mo funcionan, quÃ© entradas/salidas esperan, y guÃ­a sobre cuÃ¡ndo usar cada uno.

### ğŸ§  RAGAgent

**PropÃ³sito:**
- Proporcionar respuestas de alto nivel y contextuales buscando en la base de datos vectorial (Pinecone) fragmentos relevantes del README y sintetizando resultados con el LLM

**CuÃ¡ndo usarlo:**
- Ideal para preguntas sobre propÃ³sito del proyecto, arquitectura, configuraciÃ³n, ejemplos de uso, o cualquier tema donde el contexto derivado del README es suficiente

**Flujo de trabajo:**
- Acepta una consulta de texto libre
- Incrusta la consulta usando el embedder del proyecto (SentenceTransformer)
- Realiza una bÃºsqueda de vecinos mÃ¡s cercanos en el almacÃ©n vectorial devolviendo los k fragmentos principales
- Ensambla fragmentos recuperados en un payload de contexto consciente de relevancia y llama al LLM para producir una respuesta concisa y fundamentada en contexto

**Entradas / Salidas:**
- **Entrada**: cadena de consulta simple
- **Salida**: cadena de respuesta sintetizada (opcionalmente con citas o fragmentos de chunks recuperados)

**Limitaciones:**
- âŒ No es apta para responder preguntas que requieren datos de repositorio en tiempo real o a nivel de archivo (contenidos de archivos, lÃ­neas especÃ­ficas, historial git) â€” usa **GitHubAgent** para esas

### ğŸ”— GitHubAgent (GitHubMCPAgent + MCPTool)

**PropÃ³sito:**
- Proporcionar informaciÃ³n granular y actualizada del repositorio interactuando con un servidor GitHub MCP local expuesto vÃ­a stdio

**QuÃ© proporciona:**
- Acceso programÃ¡tico a un conjunto de herramientas expuestas por el servidor:
  - ğŸ“‚ Listado de archivos
  - ğŸ“„ Lectura de contenido de archivos
  - ğŸ” BÃºsqueda en repo
  - ğŸ“Š Diffs
  
  Las herramientas se descubren dinÃ¡micamente vÃ­a sesiÃ³n MCP

**CÃ³mo funciona:**
- Se conecta a un servidor MCP local (contenedor iniciado con Docker) usando un PAT disponible para el servidor
- Llama a `list_tools()` para enumerar capacidades MCP disponibles y envuelve cada herramienta como un MCPTool utilizable por agentes
- Cuando se invoca, un MCPTool formatea entrada de acciÃ³n similar a JSON, llama a `session.call_tool(tool_name, args)` y analiza la salida de herramienta en una observaciÃ³n utilizable para el agente

**Entradas / Salidas:**
- **Entrada**: entrada de acciÃ³n estructurada similar a JSON (campos especÃ­ficos de la herramienta) o prompts legibles por humanos delegados por un agente orquestador
- **Salida**: observaciÃ³n de herramienta sin procesar (string/JSON), post-procesada por utilidades `process_tool_output` para mantener resultados consistentes para el LLM

**Notas de seguridad y operaciÃ³n:**
- ğŸ” Requiere un `GITHUB_TOKEN` vÃ¡lido proporcionado al contenedor MCP
- â±ï¸ Las llamadas son remotas (contenedor â†” host) y pueden introducir latencia
- ğŸ’¡ Prefiere RAG para bÃºsquedas locales baratas y GitHubAgent para consultas autorizadas a nivel de archivo
- âš ï¸ Las herramientas pueden exponer operaciones sensibles; valida entradas y restringe herramientas permitidas donde sea apropiado

### ğŸ¼ Orchestrator

**PropÃ³sito:**
- Componer herramientas y LLMs en un Ãºnico agente de estilo **ReAct** que decide cuÃ¡ndo llamar a herramientas RAG o GitHub y cÃ³mo combinar resultados en una respuesta final

**Responsabilidades principales:**
- Construir un `AgentExecutor` que registre herramientas disponibles (RAGAgent, MCPTools, GitHubExecTool)
- Implementar una plantilla de prompt estructurada que impulse el bucle: **Thought** â†’ **Action** â†’ **Action Input** â†’ **Observation**
- Aplicar lÃ­mites de iteraciÃ³n y timeouts para evitar invocaciÃ³n descontrolada de herramientas
- Enrutar resultados de herramientas de vuelta al LLM y producir la respuesta final mostrada al usuario

**CÃ³mo elige herramientas:**
- El prompt instruye al LLM a:
  - âœ… Preferir **RAG** cuando README/contexto es suficiente
  - âœ… Llamar a herramientas **GitHub** cuando la pregunta requiere contenidos de archivo, lÃ­neas especÃ­ficas, o estado de repositorio activo

**IntegraciÃ³n asincrÃ³nica y runtime:**
- El Orchestrator puede combinar llamadas LLM sincrÃ³nicas y llamadas MCP asincrÃ³nicas
- La app usa un `AsyncRunner` con bucle de evento en background para ejecutar operaciones asincrÃ³nicas sin bloquear Streamlit
- La ejecuciÃ³n del agente captura y muestra errores de herramientas; el Orchestrator maneja reintentos, timeouts y fallbacks donde se configure

**Ejemplo de flujo:**

```
1ï¸âƒ£ Consulta del usuario llega al Orchestrator
   â†“
2ï¸âƒ£ Orchestrator invoca RAGAgent para recuperar contexto relevante del README
   â†“
3ï¸âƒ£ LLM analiza contexto y decide que se necesita una verificaciÃ³n a nivel de archivo
   â†’ Llama a herramienta GitHub MCP (vÃ­a GitHubAgent)
   â†“
4ï¸âƒ£ Herramienta GitHub devuelve contenido de archivo
   â†“
5ï¸âƒ£ Orchestrator envÃ­a contexto combinado al LLM para respuesta final
```

**OrientaciÃ³n:**
- âš¡ Ajusta las herramientas permitidas y el conteo de iteraciones dependiendo de consultas tÃ­picas (menos iteraciones = mÃ¡s rÃ¡pido, mÃ¡s seguro)
- ğŸ’° Usa RAGAgent para respuestas contextuales de **bajo costo**
- âœ”ï¸ Usa GitHubAgent cuando la **correcciÃ³n** y **detalles actualizados** importan
